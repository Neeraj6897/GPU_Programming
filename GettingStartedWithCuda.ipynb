{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First step into CUDA (Compute Unified Device Architecture) programming: CUDA is NVIDIA's platform for harnessing the massive parallel processing power of GPUs. This introductory notebook will guide you through the essentials: verifying your CUDA environment, understanding basic concepts like kernels, threads, and blocks, and running your first piece of code directly on a GPU."
      ],
      "metadata": {
        "id": "kkKVnotbUm-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "This notebook assumes a basic understanding of C programming. You should be comfortable with:\n",
        "\n",
        "Variables, loops, and if/else logic.\n",
        "Function definition and invocation.\n",
        "Array allocation and usage."
      ],
      "metadata": {
        "id": "-Q1GU6VfUsQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To retrieve details about this GPU, run the nvidia-smi command in your terminal. It stands for NVIDIA System Management Interface."
      ],
      "metadata": {
        "id": "gSk6nhmXWUW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tO_CPlKV4Bu",
        "outputId": "925b48f5-03c5-4251-84f6-0b73d082c680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 14 12:49:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample code for CPU and GPU"
      ],
      "metadata": {
        "id": "PuX6cnDeW7wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```cpp\n",
        "void CPUFunction()\n",
        "{\n",
        "  printf(\"This function is defined to run on the CPU.\\n\");\n",
        "}\n",
        "\n",
        "__global__ void GPUFunction()\n",
        "{\n",
        "  printf(\"This function is defined to run on the GPU.\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  CPUFunction();\n",
        "\n",
        "  GPUFunction<<<1, 1>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "}\n",
        "```\n",
        "\n",
        "`__global__ void GPUFunction()`\n",
        "- The `__global__` keyword indicates that the following function will run on the GPU, and can be invoked **globally**, which in this context means either by the CPU, or, by the GPU.\n",
        "  - Often, code executed on the CPU is referred to as **host** code, and code running on the GPU is referred to as **device** code.\n",
        "  - Notice the return type `void`. It is required that functions defined with the `__global__` keyword return type `void`.\n",
        "\n",
        "`GPUFunction<<<1, 1>>>();`\n",
        "  - Typically, when calling a function to run on the GPU, it is called as **kernel**, which is **launched**.\n",
        "  - When launching a kernel, an **execution configuration** must be provided, which is done by using the `<<< ... >>>` syntax just prior to passing the kernel any expected arguments.\n",
        "  - At a high level, execution configuration allows programmers to specify the **thread hierarchy** for a kernel launch, which defines the number of thread groupings (called **blocks**), as well as how many **threads** to execute in each block.\n",
        "  `cudaDeviceSynchronize();`\n",
        "  - Unlike much C/C++ code, launching kernels is **asynchronous**: the CPU code will continue to execute *without waiting for the kernel launch to complete*.\n",
        "  - A call to `cudaDeviceSynchronize`, a function provided by the CUDA runtime, will cause the host (CPU) code to wait until the device (GPU) code completes, and only then resume execution on the CPU."
      ],
      "metadata": {
        "id": "O7Gj69TbW3MX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - `nvcc` is the command line command for using the `nvcc` compiler.\n",
        "  - `some-CUDA.cu` is passed as the file to compile.\n",
        "  - The `o` flag is used to specify the output file for the compiled program.\n",
        "  - The `arch` flag indicates for which **architecture** the files must be compiled.\n",
        "  - Example: *nvcc -arch=sm_70 -o cuda_program cuda_program.cu*"
      ],
      "metadata": {
        "id": "EsaskDaOXF5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating and Launching Paraller Kernels\n",
        "\n",
        "The execution configuration allows programmers to specify how many groups of threads - called **thread blocks**, or just **blocks** - and how many threads they would like each thread block to contain. The syntax for this is:\n",
        "`<<< NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK>>>`\n",
        "\n",
        "**The kernel code is executed by every thread in every thread block configured when the kernel is launched**.\n",
        "\n",
        "Thus, under the assumption that a kernel called `someKernel` has been defined, the following are true:\n",
        "  - `someKernel<<<1, 1>>>()` is configured to run in a single thread block which has a single thread and will therefore run only once.\n",
        "  - `someKernel<<<1, 10>>>()` is configured to run in a single thread block which has 10 threads and will therefore run 10 times.\n",
        "  - `someKernel<<<10, 1>>>()` is configured to run in 10 thread blocks which each have a single thread and will therefore run 10 times.\n",
        "  - `someKernel<<<10, 10>>>()` is configured to run in 10 thread blocks which each have 10 threads and will therefore run 100 times."
      ],
      "metadata": {
        "id": "airqgpZJZxSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA Thread Heirarchy\n",
        "\n",
        "Each thread is given an index within its thread block, starting at `0`. Additionally, each block is given an index, starting at `0`. Just as threads are grouped into thread blocks, blocks are grouped into a **grid**, which is the highest entity in the CUDA thread hierarchy. In summary, CUDA kernels are executed in a grid of 1 or more blocks, with each block containing the same number of 1 or more threads.\n",
        "\n",
        "CUDA kernels have access to special variables identifying both the index of the thread (within the block) that is executing the kernel, and, the index of the block (within the grid) that the thread is within. These variables are `threadIdx.x` and `blockIdx.x` respectively."
      ],
      "metadata": {
        "id": "PhbJ8yPraf4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing For Loops using CUDA\n",
        "For loops in CPU-only applications are ripe for acceleration: rather than run each iteration of the loop serially, each iteration of the loop can be run in parallel in its own thread. Consider the following for loop, and notice, though it is obvious, that it controls how many times the loop will execute, as well as defining what will happen for each iteration of the loop:\n",
        "\n",
        "```cpp\n",
        "int N = 2<<20;\n",
        "for (int i = 0; i < N; ++i)\n",
        "{\n",
        "  printf(\"%d\\n\", i);\n",
        "}\n",
        "```\n",
        "In order to parallelize this loop, 2 steps must be taken:\n",
        "\n",
        "- A kernel must be written to do the work of a **single iteration of the loop**.\n",
        "- Because the kernel will be agnostic of other running kernels, the execution configuration must be such that the kernel executes the correct number of times, for example, the number of times the loop would have iterated.\n"
      ],
      "metadata": {
        "id": "GBk1PRn4cO2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the architecture of gpu: *nvidia-smi --query-gpu=compute_cap --format=csv,noheader*"
      ],
      "metadata": {
        "id": "wJBnxzDZKF8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi --query-gpu=compute_cap --format=csv,noheader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF5i3gZfKQxa",
        "outputId": "61edde83-9fd0-443a-dac8-1c46b8302453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sample_code1.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "void CPUFunction(int N)\n",
        "{\n",
        "    for (int i=0; i<N; i++)\n",
        "    {\n",
        "        printf(\"CPU: %d\\n\", i);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void GPUFunction(int N)\n",
        "{\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < N)\n",
        "    {\n",
        "        printf(\"GPU: %d\\n\", idx);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int N = 20;\n",
        "\n",
        "    CPUFunction(N);\n",
        "    GPUFunction<<<N,1>>>(N);\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LktRokt9cnoI",
        "outputId": "3b2d810b-c962-4b7b-fabf-5b0dbdbb5823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sample_code1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc sample_code1.cu -o sample_code1 -arch=sm_75"
      ],
      "metadata": {
        "id": "1DdegLagduCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./sample_code1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBQ8J7j3hShc",
        "outputId": "5714fb80-a0bb-4823-bd82-b1729c1bc1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU: 0\n",
            "CPU: 1\n",
            "CPU: 2\n",
            "CPU: 3\n",
            "CPU: 4\n",
            "CPU: 5\n",
            "CPU: 6\n",
            "CPU: 7\n",
            "CPU: 8\n",
            "CPU: 9\n",
            "CPU: 10\n",
            "CPU: 11\n",
            "CPU: 12\n",
            "CPU: 13\n",
            "CPU: 14\n",
            "CPU: 15\n",
            "CPU: 16\n",
            "CPU: 17\n",
            "CPU: 18\n",
            "CPU: 19\n",
            "GPU: 2\n",
            "GPU: 12\n",
            "GPU: 7\n",
            "GPU: 17\n",
            "GPU: 0\n",
            "GPU: 3\n",
            "GPU: 10\n",
            "GPU: 13\n",
            "GPU: 4\n",
            "GPU: 5\n",
            "GPU: 14\n",
            "GPU: 15\n",
            "GPU: 8\n",
            "GPU: 11\n",
            "GPU: 18\n",
            "GPU: 1\n",
            "GPU: 9\n",
            "GPU: 19\n",
            "GPU: 6\n",
            "GPU: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using blocks and threads for further parallelization\n",
        "\n",
        "There is a limit to the number of threads that can exist in a thread block: 1024 to be precise. In order to increase the amount of parallelism in accelerated applications, we must be able to coordinate among multiple thread blocks.\n",
        "\n",
        "CUDA Kernels have access to a special variable that gives the number of threads in a block: `blockDim.x`. Using this variable, in conjunction with `blockIdx.x` and `threadIdx.x`, increased parallelization can be accomplished by organizing parallel execution across multiple blocks of multiple threads with the idiomatic expression `threadIdx.x + blockIdx.x * blockDim.x`. Here is a detailed example.\n",
        "\n",
        "The execution configuration `<<<10, 10>>>` would launch a grid with a total of 100 threads, contained in 10 blocks of 10 threads. We would therefore hope for each thread to have the ability to calculate some index unique to itself between `0` and `99`.\n",
        "\n",
        "- If block `blockIdx.x` equals `0`, then `blockIdx.x * blockDim.x` is `0`. Adding to `0` the possible `threadIdx.x` values `0` through `9`, then we can generate the indices `0` through `9` within the 100 thread grid.\n",
        "- If block `blockIdx.x` equals `1`, then `blockIdx.x * blockDim.x` is `10`. Adding to `10` the possible `threadIdx.x` values `0` through `9`, then we can generate the indices `10` through `19` within the 100 thread grid.\n",
        "- If block `blockIdx.x` equals `5`, then `blockIdx.x * blockDim.x` is `50`. Adding to `50` the possible `threadIdx.x` values `0` through `9`, then we can generate the indices `50` through `59` within the 100 thread grid.\n",
        "- If block `blockIdx.x` equals `9`, then `blockIdx.x * blockDim.x` is `90`. Adding to `90` the possible `threadIdx.x` values `0` through `9`, then we can generate the indices `90` through `99` within the 100 thread grid."
      ],
      "metadata": {
        "id": "5otoYGpgKjFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Allocating Memory to be accessed on the GPU and the CPU\n",
        "\n",
        "More recent versions of CUDA (version 6 and later) have made it easy to allocate memory that is available to both the CPU host and any number of GPU devices, and while there are many [intermediate and advanced techniques](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations) for memory management that will support the most optimal performance in accelerated applications, the most basic CUDA memory management technique we will now cover supports fantastic performance gains over CPU-only applications with almost no developer overhead.\n",
        "\n",
        "To allocate and free memory, and obtain a pointer that can be referenced in both host and device code, replace calls to `malloc` and `free` with `cudaMallocManaged` and `cudaFree` as in the following example:\n",
        "\n",
        "```cpp\n",
        "// CPU-only\n",
        "\n",
        "int N = 2<<20;\n",
        "size_t size = N * sizeof(int);\n",
        "\n",
        "int *a;\n",
        "a = (int *)malloc(size);\n",
        "\n",
        "// Use `a` in CPU-only program.\n",
        "\n",
        "free(a);\n",
        "```\n",
        "\n",
        "```cpp\n",
        "// Accelerated\n",
        "\n",
        "int N = 2<<20;\n",
        "size_t size = N * sizeof(int);\n",
        "\n",
        "int *a;\n",
        "// Note the address of `a` is passed as first argument.\n",
        "cudaMallocManaged(&a, size);\n",
        "\n",
        "// Use `a` on the CPU and/or on any GPU in the accelerated system.\n",
        "\n",
        "cudaFree(a);\n",
        "```"
      ],
      "metadata": {
        "id": "wq-iuZdCK4t4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Block Configuration Mismatches to Number of Needed Threads\n",
        "\n",
        "Sometimes you can't configure a GPU launch to create exactly the number of threads needed for a loop. For instance, due to hardware constraints, you might want blocks with a multiple of 32 threads for efficiency. If you choose 256 threads per block but need exactly 1000 threads, there’s no integer number of blocks that gives exactly 1000 threads.\n",
        "\n",
        "To handle this, you can:\n",
        "\n",
        "Launch more threads than needed.\n",
        "\n",
        "Pass the actual work size (or total thread count needed,\n",
        "𝑁\n",
        "N) as a kernel argument.\n",
        "\n",
        "Inside the kernel, calculate the global thread index (e.g., tid + bid * bdim) and perform work only if the index is less than\n",
        "𝑁\n",
        "N.\n",
        "\n",
        "This method ensures you always have at least enough threads to cover\n",
        "𝑁\n",
        "N, with at most one extra block’s worth of threads.\n",
        "\n",
        "```cpp\n",
        "// Assume `N` is known\n",
        "int N = 100000;\n",
        "\n",
        "// Assume we have a desire to set `threads_per_block` exactly to `256`\n",
        "size_t threads_per_block = 256;\n",
        "\n",
        "// Ensure there are at least `N` threads in the grid, but only 1 block's worth extra\n",
        "size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "some_kernel<<<number_of_blocks, threads_per_block>>>(N);\n",
        "```\n",
        "\n",
        "Because the execution configuration above results in more threads in the grid than `N`, care will need to be taken inside of the `some_kernel` definition so that `some_kernel` does not attempt to access out of range data elements, when being executed by one of the \"extra\" threads:\n",
        "\n",
        "```cpp\n",
        "__global__ some_kernel(int N)\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "  if (idx < N) // Check to make sure `idx` maps to some value within `N`\n",
        "  {\n",
        "    // Only do work if it does\n",
        "  }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "QwQKGeFsLK6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gride Stride for larger datasets\n",
        "Either by choice, often to create the most performant execution configuration, or out of necessity, the number of threads in a grid may be smaller than the size of a data set. Consider an array with 1000 elements, and a grid with 250 threads (using trivial sizes here for ease of explanation). Here, each thread in the grid will need to be used 4 times. One common method to do this is to use a **grid-stride loop** within the kernel.\n",
        "\n",
        "In a grid-stride loop, each thread will calculate its unique index within the grid using `tid+bid*bdim`, perform its operation on the element at that index within the array, and then, add to its index the number of threads in the grid and repeat, until it is out of range of the array. For example, for a 500 element array and a 250 thread grid, the thread with index 20 in the grid would:\n",
        "\n",
        "- Perform its operation on element 20 of the 500 element array\n",
        "- Increment its index by 250, the size of the grid, resulting in 270\n",
        "- Perform its operation on element 270 of the 500 element array\n",
        "- Increment its index by 250, the size of the grid, resulting in 520\n",
        "- Because 520 is now out of range for the array, the thread will stop its work\n",
        "\n",
        "CUDA provides a special variable giving the number of blocks in a grid, `gridDim.x`. Calculating the total number of threads in a grid then is simply the number of blocks in a grid multiplied by the number of threads in each block, `gridDim.x * blockDim.x`. With this in mind, here is a verbose example of a grid-stride loop within a kernel:\n",
        "\n",
        "```cpp\n",
        "__global__ void kernel(int *a, int N)\n",
        "{\n",
        "  int indexWithinTheGrid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int gridStride = gridDim.x * blockDim.x;\n",
        "\n",
        "  for (int i = indexWithinTheGrid; i < N; i += gridStride)\n",
        "  {\n",
        "    // do work on a[i];\n",
        "  }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "yXtolz8wNcym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--\n",
        "## Error Handling\n",
        "\n",
        "As in any application, error handling in accelerated CUDA code is essential. Many, if not most CUDA functions (see, for example, the [memory management functions](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY)) return a value of type `cudaError_t`, which can be used to check whether or not an error occurred while calling the function. Here is an example where error handling is performed for a call to `cudaMallocManaged`:\n",
        "\n",
        "```cpp\n",
        "cudaError_t err;\n",
        "err = cudaMallocManaged(&a, N)                    // Assume the existence of `a` and `N`.\n",
        "\n",
        "if (err != cudaSuccess)                           // `cudaSuccess` is provided by CUDA.\n",
        "{\n",
        "  printf(\"Error: %s\\n\", cudaGetErrorString(err)); // `cudaGetErrorString` is provided by CUDA.\n",
        "}\n",
        "```\n",
        "\n",
        "Launching kernels, which are defined to return `void`, do not return a value of type `cudaError_t`. To check for errors occurring at the time of a kernel launch, for example if the launch configuration is erroneous, CUDA provides the `cudaGetLastError` function, which does return a value of type `cudaError_t`.\n",
        "\n",
        "```cpp\n",
        "/*\n",
        " * This launch should cause an error, but the kernel itself\n",
        " * cannot return it.\n",
        " */\n",
        "\n",
        "someKernel<<<1, -1>>>();  // -1 is not a valid number of threads.\n",
        "\n",
        "cudaError_t err;\n",
        "err = cudaGetLastError(); // `cudaGetLastError` will return the error from above.\n",
        "if (err != cudaSuccess)\n",
        "{\n",
        "  printf(\"Error: %s\\n\", cudaGetErrorString(err));\n",
        "}\n",
        "```\n",
        "\n",
        "Finally, in order to catch errors that occur asynchronously, for example during the execution of an asynchronous kernel, it is essential to check the status returned by a subsequent synchronizing CUDA runtime API call, such as `cudaDeviceSynchronize`, which will return an error if one of the kernels launched previously should fail."
      ],
      "metadata": {
        "id": "0xpb-vZYNwlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### CUDA Error Handling Function\n",
        "\n",
        "It can be helpful to create a macro that wraps CUDA function calls for checking errors. Here is an example, feel free to use it in the remaining exercises:\n",
        "\n",
        "```cpp\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "inline cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "  return result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "/*\n",
        " * The macro can be wrapped around any function returning\n",
        " * a value of type `cudaError_t`.\n",
        " */\n",
        "\n",
        "  checkCuda( cudaDeviceSynchronize() )\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "3LvF44zjN5qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grids and Blocks of 2 and 3 Dimensions\n",
        "\n",
        "Grids and blocks can be defined to have up to 3 dimensions. Defining them with multiple dimensions does not impact their performance in any way, but can be very helpful when dealing with data that has multiple dimensions, for example, 2d matrices. To define either grids or blocks with two or 3 dimensions, use CUDA's `dim3` type as such:\n",
        "\n",
        "```cpp\n",
        "dim3 threads_per_block(16, 16, 1);\n",
        "dim3 number_of_blocks(16, 16, 1);\n",
        "someKernel<<<number_of_blocks, threads_per_block>>>();\n",
        "```\n",
        "\n",
        "Given the example just above, the variables `gridDim.x`, `gridDim.y`, `blockDim.x`, and `blockDim.y` inside of `someKernel`, would all be equal to `16`."
      ],
      "metadata": {
        "id": "CBPGaVtsOBQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_add.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * Host function to initialize vector elements. This function\n",
        " * simply initializes each element to equal its index in the\n",
        " * vector.\n",
        " */\n",
        "\n",
        "__global__\n",
        "void initWith(float num, float *a, int N)\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = idx; i < N; i += stride)\n",
        "  {\n",
        "    a[i] = num;\n",
        "  }\n",
        "}\n",
        "\n",
        "/*\n",
        " * Device kernel stores into `result` the sum of each\n",
        " * same-indexed value of `a` and `b`.\n",
        " */\n",
        "\n",
        "__global__\n",
        "void addVectorsInto(float *result, float *a, float *b, int N)\n",
        "{\n",
        "  int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < N; i += stride)\n",
        "  {\n",
        "    result[i] = a[i] + b[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "/*\n",
        " * Host function to confirm values in `vector`. This function\n",
        " * assumes all values are the same `target` value.\n",
        " */\n",
        "\n",
        "void checkElementsAre(float target, float *vector, int N)\n",
        "{\n",
        "  for(int i = 0; i < N; i++)\n",
        "  {\n",
        "    if(vector[i] != target)\n",
        "    {\n",
        "      printf(\"FAIL: vector[%d] - %0.0f does not equal %0.0f\\n\", i, vector[i], target);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "  printf(\"Success! All values calculated correctly.\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  const int N = 2<<24;\n",
        "  size_t size = N * sizeof(float);\n",
        "\n",
        "  float *a;\n",
        "  float *b;\n",
        "  float *c;\n",
        "\n",
        "  cudaMallocManaged(&a, size);\n",
        "  cudaMallocManaged(&b, size);\n",
        "  cudaMallocManaged(&c, size);\n",
        "\n",
        "  size_t threadsPerBlock;\n",
        "  size_t numberOfBlocks;\n",
        "\n",
        "  /*\n",
        "   * nsys should register performance changes when execution configuration\n",
        "   * is updated.\n",
        "   */\n",
        "\n",
        "  threadsPerBlock = 32;\n",
        "  numberOfBlocks = (threadsPerBlock+N-1)/threadsPerBlock;\n",
        "\n",
        "  //printf(\"%d \\n\", numberOfBlocks);\n",
        "\n",
        "  int deviceId;\n",
        "  cudaGetDevice(&deviceId);\n",
        "\n",
        "  cudaDeviceProp props;\n",
        "\n",
        "  cudaGetDeviceProperties(&props, deviceId);\n",
        "\n",
        "  int multiProcessorCount = props.multiProcessorCount;\n",
        "\n",
        "  int temp = numberOfBlocks % multiProcessorCount;\n",
        "  if(temp != 0) {\n",
        "      numberOfBlocks = numberOfBlocks + multiProcessorCount - temp;\n",
        "     }\n",
        "\n",
        "  //printf(\"%d \\n\", numberOfBlocks);\n",
        "\n",
        "  initWith<<<numberOfBlocks, threadsPerBlock>>>(3, a, N);\n",
        "  initWith<<<numberOfBlocks, threadsPerBlock>>>(4, b, N);\n",
        "  initWith<<<numberOfBlocks, threadsPerBlock>>>(0, c, N);\n",
        "\n",
        "  cudaError_t addVectorsErr;\n",
        "  cudaError_t asyncErr;\n",
        "\n",
        "  cudaMemPrefetchAsync(a, size, deviceId);\n",
        "  cudaMemPrefetchAsync(b, size, deviceId);\n",
        "  cudaMemPrefetchAsync(c, size, deviceId);\n",
        "\n",
        "  addVectorsInto<<<numberOfBlocks, threadsPerBlock>>>(c, a, b, N);\n",
        "\n",
        "  addVectorsErr = cudaGetLastError();\n",
        "  if(addVectorsErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(addVectorsErr));\n",
        "\n",
        "  asyncErr = cudaDeviceSynchronize();\n",
        "  if(asyncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr));\n",
        "\n",
        "  cudaMemPrefetchAsync(c, size, cudaCpuDeviceId);\n",
        "\n",
        "  checkElementsAre(7, c, N);\n",
        "\n",
        "  cudaFree(a);\n",
        "  cudaFree(b);\n",
        "  cudaFree(c);\n",
        "}\n"
      ],
      "metadata": {
        "id": "xaroFQXiPPQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3178f4-dd53-4d5d-b8ba-0b1c5093cd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vector_add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unified Memory Migration\n",
        "\n",
        "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
        "\n",
        "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
        "\n",
        "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
        "\n",
        "`nsys profile` provides output describing UM behavior for the profiled application.\n",
        "\n",
        "In the output of `nsys profile --stats=true` we should be looking for the following:\n",
        "\n",
        "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
        "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
        "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
      ],
      "metadata": {
        "id": "6SN26S8QI-3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asynchronous Memory Prefetching\n",
        "\n",
        "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
        "\n",
        "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
        "\n",
        "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
        "\n",
        "```cpp\n",
        "int deviceId;\n",
        "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
        "\n",
        "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
        "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
        "                                                                  // built-in CUDA variable.\n",
        "```"
      ],
      "metadata": {
        "id": "UkRgqB8WJrps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concurrent CUDA Streams\n",
        "CUDA streams are like independent lanes on a highway for GPU tasks. They allow you to run multiple tasks concurrently (or overlap tasks such as computation and memory transfers), which can improve overall performance by hiding latencies.\n",
        "In Simple Terms:\n",
        "#####Single Stream:\n",
        "* If you use one stream (the default stream), tasks (like kernel executions and memory copies) run one after another.\n",
        "\n",
        "#####Multiple Streams:\n",
        "* By creating multiple streams, you can submit different tasks to different lanes. The GPU can then execute tasks in parallel if they don’t depend on one another.\n",
        "\n",
        "#### A Simple Example:\n",
        "Imagine you want to perform two independent computations concurrently. Here's a small CUDA C/C++ example:\n",
        "\n",
        "```cpp\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "// A simple kernel that fills an array with its index values.\n",
        "__global__ void fillKernel(int *data, int value) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    data[idx] = value;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int arraySize = 1024;\n",
        "    const int arrayBytes = arraySize * sizeof(int);\n",
        "\n",
        "    // Allocate device memory.\n",
        "    int *d_data;\n",
        "    cudaMalloc(&d_data, arrayBytes);\n",
        "\n",
        "    // Create two CUDA streams.\n",
        "    cudaStream_t stream1, stream2;\n",
        "    cudaStreamCreate(&stream1);\n",
        "    cudaStreamCreate(&stream2);\n",
        "\n",
        "    // Launch the kernel in stream1 to fill the array with 1's.\n",
        "    fillKernel<<<4, 256, 0, stream1>>>(d_data, 1);\n",
        "    // Launch the kernel in stream2 to fill the array with 2's.\n",
        "    fillKernel<<<4, 256, 0, stream2>>>(d_data, 2);\n",
        "\n",
        "    // Wait for both streams to finish.\n",
        "    cudaStreamSynchronize(stream1);\n",
        "    cudaStreamSynchronize(stream2);\n",
        "\n",
        "    // Clean up.\n",
        "    cudaStreamDestroy(stream1);\n",
        "    cudaStreamDestroy(stream2);\n",
        "    cudaFree(d_data);\n",
        "\n",
        "    printf(\"Kernels executed in parallel using CUDA streams.\\n\");\n",
        "    return 0;\n",
        "}\n",
        "```\n",
        "###Explanation:\n",
        "1. Memory Allocation:\n",
        "Device memory is allocated for an array.\n",
        "\n",
        "2. Stream Creation:\n",
        "Two streams (stream1 and stream2) are created.\n",
        "\n",
        "3. Kernel Launches:\n",
        "The fillKernel is launched twice:\n",
        "\n",
        "4. Once on stream1, filling the array with the value 1.\n",
        "\n",
        "5. Once on stream2, filling the array with the value 2.\n",
        "\n",
        "6. Because these streams are independent, the GPU may execute both kernels concurrently.\n",
        "\n",
        "7. Synchronization:\n",
        "The program waits until both streams are finished before cleaning up, ensuring all operations are complete.\n",
        "\n",
        "By using streams, you can leverage the GPU’s ability to perform tasks concurrently, which can be beneficial for overlapping computation with data transfer, or for running multiple independent computations at once."
      ],
      "metadata": {
        "id": "g7zcbqu3J6GM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rules Governing the Behavior of CUDA Streams\n",
        "\n",
        "There are a few rules, concerning the behavior of CUDA streams, that should be learned in order to utilize them effectively:\n",
        "\n",
        "- Operations within a given stream occur in order.\n",
        "- Operations in different non-default streams are not guaranteed to operate in any specific order relative to each other.\n",
        "- The default stream is blocking and will both wait for all other streams to complete before running, and, will block other streams from running until it completes."
      ],
      "metadata": {
        "id": "cbzR3f6GOscJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5FbW0qwsTdQv",
        "outputId": "d7d14b20-2ca3-4c19-9f42-47ee9de06f55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    }
  ]
}