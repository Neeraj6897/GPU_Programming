{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4d12dd",
   "metadata": {},
   "source": [
    "Here’s a concise expert-level breakdown of Reduction #5 (“Unroll the Last Warp”)\n",
    "\n",
    "## Summary of Key Improvements  \n",
    "When the stride `s` drops to 32 or below, only one warp (32 threads) remains active. At that point:  \n",
    "- **No need for `__syncthreads()`**—threads in a warp execute in lock-step.  \n",
    "- **No need for `if (tid < s)`**—every thread in the warp has valid work.  \n",
    "- **Unroll the last six iterations** manually to remove loop overhead and branch instructions.  \n",
    "- **Mark shared memory as `volatile`** in the warp-unroll routine so register-to-register updates become visible to all threads without additional barriers.  \n",
    "\n",
    "These changes cut ancillary instruction overhead (address arithmetic, branching, loop control) and boost bandwidth from ~17 GB/s to ~31 GB/s (for a 4 M-element reduction), halving the kernel time from ~0.97 ms to ~0.536 ms—a 1.8× speedup over Version 4 and ~15× over the naïve version.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Original Tail-Loop (Version 4)\n",
    "\n",
    "```cpp\n",
    "for (unsigned int s = blockDim.x/2; s > 0; s >>= 1) {\n",
    "  if (tid < s) {\n",
    "    sdata[tid] += sdata[tid + s];\n",
    "  }\n",
    "  __syncthreads();\n",
    "}\n",
    "```\n",
    "- When `s <= 32`, only threads with `tid < s` (half the warp, then fewer) do work; others idle, yet still execute the loop and branch—wasting cycles.  \n",
    "- Each iteration still pays the cost of loop control, branch evaluation, and (for `s > 32`) a full `__syncthreads()`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Unrolled Warp-Reduce Routine\n",
    "\n",
    "```cpp\n",
    "__device__ void warpReduce(volatile int* sdata, int tid) {\n",
    "  sdata[tid]     += sdata[tid + 32];\n",
    "  sdata[tid]     += sdata[tid + 16];\n",
    "  sdata[tid]     += sdata[tid + 8];\n",
    "  sdata[tid]     += sdata[tid + 4];\n",
    "  sdata[tid]     += sdata[tid + 2];\n",
    "  sdata[tid]     += sdata[tid + 1];\n",
    "}\n",
    "```\n",
    "\n",
    "### Why Each Line Matters  \n",
    "1. **Unroll six steps**: These correspond to strides 32, 16, 8, 4, 2, 1—the remaining reduction after the main loop.  \n",
    "2. **Remove branches**: No `if (tid < s)`—all 32 threads perform these additions in lock-step, no divergence.  \n",
    "3. **No `__syncthreads()`**: Within a warp, threads are implicitly synchronized at each instruction.  \n",
    "4. **Use `volatile int* sdata`**: Ensures each write to `sdata[...]` is immediately visible to other threads in the warp, preventing the compiler or hardware from re-ordering or caching these stores in registers.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Integrated Loop + Warp-Unroll\n",
    "\n",
    "```cpp\n",
    "// Main reduction loop handles strides > 32\n",
    "for (unsigned int s = blockDim.x/2; s > 32; s >>= 1) {\n",
    "  if (tid < s) {\n",
    "    sdata[tid] += sdata[tid + s];\n",
    "  }\n",
    "  __syncthreads();\n",
    "}\n",
    "\n",
    "// Last warp: unrolled, volatile, no sync or branch\n",
    "if (tid < 32) {\n",
    "  warpReduce(sdata, tid);\n",
    "}\n",
    "```\n",
    "\n",
    "### What Changed  \n",
    "- **Loop condition `s > 32`**: Stop the generic loop once only one warp remains.  \n",
    "- **Single `if (tid < 32)`**: Only threads in that final warp enter `warpReduce`. This is a single uniform branch per warp.  \n",
    "- **All work inside `warpReduce`** uses no further synchronization or branching.\n",
    "\n",
    "---\n",
    "\n",
    "## Dry Run Example (Block Size 128):\n",
    "\n",
    "Initial Loop (s > 32):\n",
    "\n",
    "s starts at 64 (128 / 2).\n",
    "\n",
    "Iteration 1 (s = 64): Threads 0-63 add sdata[tid + 64] to sdata[tid]. __syncthreads() ensures all additions complete.\n",
    "\n",
    "Iteration 2 (s = 32): The loop terminates because s is no longer greater than 32.\n",
    "\n",
    "if (tid < 32) warpReduce(sdata, tid);:\n",
    "\n",
    "Threads 0-31 (the first warp) execute warpReduce. At this point, sdata[0] to sdata[63] hold the pairwise sums of the original 128 elements. warpReduce then sums the first 32 of these partial sums within the first warp.\n",
    "\n",
    "## 4. Dry-Run Example (BlockDim = 8 → warp size = 8 for illustration)\n",
    "\n",
    "Assume a small “warp” of 8 threads and unroll the last three steps (strides 4, 2, 1):\n",
    "\n",
    "Initial shared data after first stages:  \n",
    "```\n",
    "tid:    0   1   2   3   4   5   6   7\n",
    "sdata: [A,  B,  C,  D,  E,  F,  G,  H]\n",
    "```\n",
    "\n",
    "1. **warpReduce step for stride=4**:  \n",
    "   All tids 0–3 do `sdata[tid]+=sdata[tid+4]`:  \n",
    "   ```\n",
    "   tid=0: A+=E → A'\n",
    "   tid=1: B+=F → B'\n",
    "   tid=2: C+=G → C'\n",
    "   tid=3: D+=H → D'\n",
    "   ```\n",
    "2. **stride=2**:  \n",
    "   tids 0–1 do `sdata[tid]+=sdata[tid+2]`:  \n",
    "   ```\n",
    "   tid=0: A'+=C' → A''\n",
    "   tid=1: B'+=D' → B''\n",
    "   ```\n",
    "3. **stride=1**:  \n",
    "   tid=0 does `sdata[0]+=sdata[1]`:  \n",
    "   ```\n",
    "   A''' = A'' + B''\n",
    "   ```\n",
    "\n",
    "No threads ever idle during these unrolled steps, and no barriers are needed.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Role of `volatile`\n",
    "\n",
    "- **Without `volatile`**, the compiler or GPU may keep `sdata[tid]` in a register across those six lines, or reorder accesses, causing later threads in the warp to read stale values from shared memory.  \n",
    "- Marking the pointer `volatile int* sdata` tells the compiler and hardware: “Always perform each read/write exactly as written, in order, to shared memory,” ensuring correctness in this fine-grained, warp-synchronous code.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Overall Impact\n",
    "\n",
    "| Step                          | Time (4 M elems) | Bandwidth   | Speedup vs Prev |\n",
    "|-------------------------------|------------------|-------------|----------------:|\n",
    "| Version 4 (first-add)         | 0.965 ms         | 17.4 GB/s   | 1×              |\n",
    "| **Version 5 (warp unroll)**   | **0.536 ms**     | **31.3 GB/s** | **1.8×**         |\n",
    "\n",
    "By unrolling the final warp and using `volatile`, you eliminate loop overhead, branching, and unnecessary barriers—unlocking the full shared-memory bandwidth for this memory-bound reduction.\n",
    "\n",
    "When you unroll the last warp, you stop the loop early (so threads skip s≤32 iterations) and replace it with one `if(tid<32)` plus straight-line adds—so all other threads avoid repeated branch checks and barriers. This eliminates wasted loop‐control and synchronization overhead across every warp, not just the final one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2dee9",
   "metadata": {},
   "source": [
    "## Additional Notes:\n",
    "\n",
    "__global__ and __device__ are both CUDA qualifiers that tell the compiler where a function lives (on the GPU) and who may call it. Here’s the core difference in two lines—and then a bit more detail:  \n",
    "\n",
    "- **`__global__`** marks a **kernel**: a function you launch from the **host** (CPU) with the `<<<…>>>` syntax, and which executes on the **device** (GPU).  \n",
    "- **`__device__`** marks a **GPU helper**: a function that lives on the **device** and can only be called by other GPU code (i.e. by `__global__` kernels or by other `__device__` functions).  \n",
    "\n",
    "---\n",
    "\n",
    "## Qualifier Meanings  \n",
    "\n",
    "| Qualifier    | Callable From     | Executes On | Notes                                             |\n",
    "|--------------|-------------------|-------------|---------------------------------------------------|\n",
    "| `__global__` | Host only         | Device      | Entry-point (“kernel”), must return `void`, uses `<<<…>>>` launch syntax . |\n",
    "| `__device__` | Device (GPU) only | Device      | GPU-only function, used for sharing logic between kernels or decomposing work.|\n",
    "\n",
    "---\n",
    "\n",
    "## Why the Distinction Matters  \n",
    "\n",
    "1. **Entry-point ABI**  \n",
    "   - A `__global__` kernel generates the glue code so the CPU can schedule work on the GPU. Without `__global__`, you can’t launch from host.  \n",
    "2. **Call-graph Restrictions**  \n",
    "   - `__device__` functions are inlined or called by GPU code only; they are not visible to host code and incur no host-launch overhead.  \n",
    "3. **Overhead Differences**  \n",
    "   - Calling a `__global__` kernel involves driver/API overhead to dispatch work to the GPU. A `__device__` call is a simple function call within GPU execution, with far lower overhead.  \n",
    "\n",
    "---\n",
    "\n",
    "## Simple Example  \n",
    "\n",
    "```cpp\n",
    "// A device helper: can only be called by GPU code\n",
    "__device__ float square(float x) {\n",
    "    return x * x;\n",
    "}\n",
    "\n",
    "// A global kernel: launched from host, runs on GPU\n",
    "__global__ void computeSquares(float *data, int N) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < N) {\n",
    "        // call to device function:\n",
    "        data[idx] = square(data[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Host code:\n",
    "int main() {\n",
    "    // … allocate GPU memory into d_data …\n",
    "    computeSquares<<<grid, block>>>(d_data, N);  // __global__ launch\n",
    "    // … copy back and cleanup …\n",
    "}\n",
    "```\n",
    "\n",
    "- `computeSquares` must be declared `__global__` so the CPU can launch it on the GPU.  \n",
    "- `square` is `__device__` because it runs on the GPU and is only used by the kernel.  \n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways  \n",
    "\n",
    "- Use **`__global__`** for any function you want to call **from the CPU** to run on the GPU.  \n",
    "- Use **`__device__`** for helper routines that are purely **GPU-side** and only called by other GPU functions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
